{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a68d55-5e9b-4bab-b324-05b89a56cecd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import abtem\n",
    "import ase\n",
    "import dask\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c74ed-dfb6-4426-898a-63110b6fb253",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "(walkthrough:parallelization)=\n",
    "# Parallelization\n",
    "\n",
    "The computational cost of running multislice simulations can become high depending on the size of the specimen, the number of probe positions, phonon images, and many other factors. This cost can be mitigated using parallelism. Many of the computations in *ab*TEM are [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel): for example, every probe position is independent, thus each CPU core may calculate a batch of positions independently, only requiring communication after finishing a run of the multislice algorithm. \n",
    "\n",
    "*ab*TEM is parallelized using [Dask](https://www.dask.org/){cite}`dask`, which is a flexible library for parallel computing in Python. Dask allows scaling from a single laptop to hundreds of nodes at high-performance computing (HPC) facilities with minimal changes to the code. \n",
    "\n",
    "In this walkthrough, we introduce how *ab*TEM uses Dask. Althouhg this is not required knowledge for running simulations on a single machine, it may still help you optimize your simulations. If you are already an experienced Dask user, most of what you already know can be applied to using *ab*TEM. If you are new to Dask you may benefit from watching [this introduction](https://www.youtube.com/watch?v=nnndxbr_Xq4) before continuing.\n",
    "\n",
    "We note that Dask is used in several other libraries for the analysis of electron microscopy data, for example, [hyperspy](https://hyperspy.org/), [libertem](https://libertem.github.io/LiberTEM/) and [py4DSTEM](https://py4dstem.readthedocs.io/en/latest/), and thus we think that you may benefit from knowing this library more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2aaf0-80a0-4058-99a2-faa12025ab42",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task graphs\n",
    "\n",
    "Simulating TEM experiments requires executing multiple *tasks* where each one may depend on the output of previous tasks. In Dask this is represented as a [*task graph*](https://docs.dask.org/en/stable/graphs.html), where each task is a *node*, with *edges* between nodes if it is dependent on another task. The simulation result is obtained by executing each task (node) in the graph with a Dask *scheduler* on a single machine or a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6765fb-d0ea-4c35-b5bf-2eaadee69b0b",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(url=\"https://docs.dask.org/en/stable/_images/dask-overview.svg\", width=600)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b10156-ee85-4600-92ed-b9c7c398dbdc",
   "metadata": {},
   "source": [
    "To illustrate this in practice, below we create the task graph for running a plane wave multislice simulation of gold in the $\\left<100\\right>$ zone axis with 4 frozen phonon configurations (please refer to previous walkthroughs for those details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba5ba0-0d6d-4ecb-8fdd-fa80316a579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = ase.build.bulk(\"Au\", cubic=True) * (5, 5, 2)\n",
    "\n",
    "frozen_phonons = abtem.FrozenPhonons(\n",
    "    atoms, num_configs=4, sigmas=0.1, ensemble_mean=False\n",
    ")\n",
    "\n",
    "potential = abtem.Potential(frozen_phonons, gpts=512, slice_thickness=2)\n",
    "\n",
    "probe = abtem.PlaneWave(energy=200e3)\n",
    "\n",
    "exit_waves = probe.multislice(potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc97cd9-2777-4ef4-a0c4-c020517b708e",
   "metadata": {},
   "source": [
    "The result is an ensemble of $4$ wave functions of shape $512\\times512$, which may be represented as a 3D array, where the first dimension represents the phonon ensemble and the last two dimensions represents the 2D wave functions. \n",
    "\n",
    "As we have not executed the task graph yet, the wave functions are represented as a [Dask Array](https://docs.dask.org/en/stable/array.html). We can think of the *Dask array* as being composed of many smaller NumPy arrays, called *chunks*, and operations may be applied to each chunk rather than the full array. This enables: \n",
    "\n",
    "1. Parallelism over the chunks;\n",
    "2. Representing a larger-than-memory array as many smaller arrays, each of which fits in memory.\n",
    "\n",
    "The Dask array representation `__repr__` shows how the chunks are laid out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc736d7-6ca1-47a7-baa5-eb6a9b0d69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_waves.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d22d90-2701-4d3e-bc91-ab2260232950",
   "metadata": {},
   "source": [
    "We see that this Dask array has the shape `(4, 512, 512)` requiring 8 MB of memory, and is composed of chunks with shape `(1, 512, 512)` requiring 2 MB each.\n",
    "\n",
    "```{important}\n",
    "The Dask array just represents a task graph, not the data itself, so memory is consumed only if it is computed!\n",
    "```\n",
    "\n",
    "Each chunk of the Dask array created above represents a wave function for a frozen phonon configuration. This reflects the fact that, in the multislice algorithm, each frozen phonon configuration is independent and may be calculated in parallel. On the other hand, we should not have chunks across wave functions, because each part of a wave function is affected by every other part.\n",
    "\n",
    "We can visualize the task graph using Dask's [`visualize`](https://docs.dask.org/en/stable/graphviz.html) method. We see that the task graph consists of 4 fully independent branches, one for each frozen phonon.\n",
    "\n",
    "````{note}\n",
    "Drawing Dask task graphs with the cytoscape engine requires the `ipycytoscape` python library. To reproduce the result below you need to:\n",
    "```\n",
    "  python -m pip install ipycytoscape\n",
    "```\n",
    "and restart jupyter.\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d463ae-c282-4710-a05c-0817cb996457",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "exit_waves.array.visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d970c0-56e5-4066-b1d8-f6c71f59fb46",
   "metadata": {},
   "source": [
    "We usually want to take the mean across the frozen phonon dimension, and thus end up with an image represented as a single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e3a1e-7132-4dc5-aa9b-bdeed01a6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrtem_image = exit_waves.intensity().mean(0)\n",
    "\n",
    "hrtem_image.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9661725-7a20-44f6-8415-52489e91259b",
   "metadata": {},
   "source": [
    "Taking the mean across frozen phonon chunks requires communicating the exit wave function intensity between the final nodes. Showing the task graph again we see how the branches are merged when the result have to be communicated between workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8b98c-362e-4543-b6c2-e033a827143e",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "hrtem_image.array.visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdbfb7-41d3-49ff-995f-bf17f01a120f",
   "metadata": {},
   "source": [
    "## Chunks\n",
    "To futher explore the role of chunks in abTEM, we create the task graph for running a STEM simulation with the same specimen, gold in the $\\left<100\\right>$ zone axis with 4 frozen phonons. We do not immediately apply a detector, so we obtain an ensemble of exit wave functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9baf98-b088-4c6f-954d-8ae1a7d22c23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probe = abtem.Probe(energy=200e3, semiangle_cutoff=20)\n",
    "\n",
    "scan = abtem.GridScan(\n",
    "    start=(0, 0),\n",
    "    end=(1 / 5, 1 / 5),\n",
    "    fractional=True,\n",
    "    potential=potential,\n",
    ")\n",
    "\n",
    "frozen_phonons = abtem.FrozenPhonons(\n",
    "    atoms, num_configs=4, sigmas=0.1, ensemble_mean=True\n",
    ")\n",
    "\n",
    "potential = abtem.Potential(frozen_phonons, gpts=512, slice_thickness=2)\n",
    "\n",
    "exit_waves_stem = probe.multislice(potential, scan=scan)\n",
    "\n",
    "exit_waves_stem.axes_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69036139-511e-482d-8f72-e66b71df0924",
   "metadata": {},
   "source": [
    "The wave functions are now represented as a 5D Dask array, a 3D ensemble of 2D wave functions, which is composed of one frozen phonon dimension and two scan dimensions, one for each of the $x$ and $y$ direction. \n",
    "\n",
    "The full array is of shape `(4, 15, 15, 512, 512)` requiring 1.76 GB of memory, which is cut into chunks of shape `(1, 8, 7, 512, 512)` of 112 MB. Hence, there are a total of $4 \\times 2 \\times 3 = 24$ chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ab17b-53e3-4e79-856a-d734fe3c1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_waves_stem.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00985534-3b37-458c-9cba-e5f6c783f334",
   "metadata": {},
   "source": [
    "Notice that we do not make a chunk for every probe position, but each chunk of the scan dimension instead represents a batch of wave functions. This is done partly to limit [the overhead](https://docs.dask.org/en/stable/best-practices.html#avoid-very-large-graphs) that every chunk comes with, but more importantly, larger batches enables efficient thread parallelization within each run of the multislice algorithm.\n",
    "\n",
    "We can change how many wave functions each batch should have using a keyword. Below we set `max_batch=4`, resulting in a total number of $4 \\times 8 \\times 8 = 256$ chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9437b-ed6a-4836-8e6c-daa0ae526b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_waves_stem = probe.multislice(potential, scan=scan, max_batch=4)\n",
    "\n",
    "exit_waves_stem.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28998800-0b81-49f7-b3c6-79aa355cf3dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The default value of `max_batch` is `\"auto\"`: with this setting the number of wave functions in each batch is determined such that the batch represents approximately `128 MB` of memory, although this number may be changed through the *ab*TEM [configuration](../../reference/config.md).\n",
    "\n",
    "Before computing this task graph, we apply a HAADF detector and calculate the ensemble mean, which reduces the total size of the output to just 900 B. We note that the 1.76 GB ensemble of wave functions never needs to be in memory simulateneously, as each chunk of exit wave functions are reduced immediately after completing the multislice algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664e8b2-4928-4c5e-bd48-fa854a465cec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "detector = abtem.AnnularDetector(inner=65, outer=200)\n",
    "\n",
    "haadf_images = detector.detect(exit_waves_stem).reduce_ensemble()\n",
    "\n",
    "haadf_images.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8167e51-1e8b-408d-9e08-ad00b6966ad5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Schedulers\n",
    "\n",
    "After generating a task graph, it needs to be executed on (parallel) hardware. This is the job of a [task scheduler](https://docs.dask.org/en/stable/scheduler-overview.html). Dask provides several *task schedulers*: each of which will compute a task graph and give the same result, but with different performance characteristics.\n",
    "\n",
    "Every time you call the `compute` method a Dask scheduler is used. We adopt the default Dask scheduler configuration and every keyword argument used with the `compute` method in *ab*TEM is forwarded to the Dask `compute` function. \n",
    "\n",
    "### Local scheduler\n",
    "\n",
    "The default scheduler is the [`ThreadPoolExecutor`](https://docs.dask.org/en/stable/scheduling.html#local-threads). As an example of a keyword argument, the threaded scheduler takes `num_workers`, which sets the number of threads to use (by default, up to the number of computing cores). Let's run compute with 8 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c507a96-5a6d-4e55-8775-200f66e26459",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "haadf_images = detector.detect(exit_waves_stem).reduce_ensemble()\n",
    "\n",
    "haadf_images.compute(scheduler=\"threads\", num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c4090-255a-4705-85a8-afbf74c6320e",
   "metadata": {},
   "source": [
    "We can change the scheduler to using the `ProcessPoolExecutor` as below.\n",
    "\n",
    "```python\n",
    "haadf_images = detector.detect(exit_waves_stem).compute(scheduler=\"processes\", num_workers=4)\n",
    "```\n",
    "\n",
    "Using `abtem.config.set` the scheduler can be set either as a context manager or globally.\n",
    "```python\n",
    "# As a context manager\n",
    "with abtem.config.set(scheduler=\"processes\"):\n",
    "    haadf_images.compute()\n",
    "\n",
    "# Set globally\n",
    "abtem.config.set(scheduler=\"processes\")\n",
    "haadf_images.compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a4c6c-338c-4c3a-90e2-8ad79ed63dd4",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945988ec-0d2b-422a-a84c-32f10d5a63af",
   "metadata": {},
   "source": [
    "To improve performance, we have to be able to profile it. Profiling parallel code can be challenging, but Dask provides functionality to aid in this and inspecting execution. The diagnostic tools are however quite different depending on whether you use a local or distributed scheduler.\n",
    "\n",
    "Dask allows local diagnostics by adding callbacks that collect information about your code execution. You can use the profilers as a context manager as described in the [Dask documentation](https://docs.dask.org/en/stable/diagnostics-local.html). For convenience, the *ab*TEM `compute` methods also implement keywords for adding Dask profilers.\n",
    "\n",
    "Below we use the [`Profiler`](https://docs.dask.org/en/stable/diagnostics-local.html#profiler) to monitor task execution by setting `profiler=True` and a [`ResourceProfiler`](https://docs.dask.org/en/stable/diagnostics-local.html#resourceprofiler) to monitor the CPU usage and memory consumption by setting `resource_profiler=True`. We rerun the simulation above with these profilers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb3754-6cde-4f83-b058-6014356cefaf",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "haadf_images = detector.detect(exit_waves_stem).reduce_ensemble()\n",
    "\n",
    "haadf_images, profilers = haadf_images.compute(profiler=True, resource_profiler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b63fd8-4e8b-4842-9c15-2fa8787da397",
   "metadata": {},
   "source": [
    "To display the results Dask uses the [Bokeh plotting library](https://bokeh.org/) (which is installed together with Dask). To display the plots in a Jupyter notebook we need to run the commands below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a100cb-9792-45bb-b855-42b53fb9db83",
   "metadata": {},
   "source": [
    "We first show the result from the `Profiler` object: This shows the execution time for each task as a rectangle, organized along the vertical axis by worker (in this case threads), and where white space represents times the worker was idle. The task types are grouped by color and, by hovering over each task, one can see the key and task that each block represents. For this calculation there is only one significant task shown in yellow; this encompasses building the wave function, running the multislice algorithm (calculating the potential on demand), and calculating and integrating the diffraction patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a420e98-6235-4e01-b079-fe34d913a436",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "profilers[0].visualize();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7afda6-5b1d-4362-9495-4ad140716f5f",
   "metadata": {},
   "source": [
    "The result from the `ResourceProfiler` is shown below: This shows two lines, one for total CPU percentage used by all the workers, and one for total memory usage. The CPU usage is scaled so each worker contributes up to $100 \\ \\%$, i.e. two fully utilized workers use $200 \\ \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42497cde-4e5c-4844-91e5-e8d91e1aafa9",
   "metadata": {
    "tags": [
     "skip-test"
    ]
   },
   "outputs": [],
   "source": [
    "profilers[1].visualize();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9fa0a-f044-40cd-8964-9e4a1911fa87",
   "metadata": {},
   "source": [
    "We ran the calculation on a single computer with an 4-core CPU with 2 threads per core for 8 threads total. We see that our peak CPU usage was $\\sim 600 \\%$, this is a fairly typical usage statistic for a single machine, overhead and background processes limited us from reaching the $800 \\ \\%$ corresponding to using every available thread maximally.\n",
    "\n",
    "We also see that the total memory use reached around 800 MB. If you are running the calculation on a system with more threads your memory consumption may be larger – it may even exceed the total memory cost of all the wave functions, as every parallel run of the multislice algorithm requires a significant overhead for intermediate results (such as potential slices and Fresnel propagators). If your calculation runs out of memory you can lower the number of workers, thus trading away computational speed for lower memory consumption.\n",
    "\n",
    "We note that the overhead in both CPU usage and memory diminishes for larger simulation with more powerful hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a96448-5a2a-4c60-9ce7-7a2f43845d3c",
   "metadata": {},
   "source": [
    "### The (locally) distributed scheduler\n",
    "The Dask distributed scheduler is necessary for running your simulations on a cluster, but it also runs [locally on a personal machine](https://docs.dask.org/en/stable/scheduling.html#dask-distributed-local). You can find details in the Dask documentation, but we demonstrate the basics below.\n",
    "\n",
    "You can use the Dask distributed scheduler by just initializing a [Dask Client](https://distributed.dask.org/en/stable/client.html). The `Client` takes keyword arguments such as `n_workers` (note that this is different from `num_workers` above!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c37a3-909c-482e-99fb-72e4eb222a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=6)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cacd8-4c48-4eea-930c-33f3fe834410",
   "metadata": {},
   "source": [
    "After intializing the client object, any *ab*TEM computation will use the Dask distributed scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d7e8f-24dd-4ada-9650-874e750a3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "haadf_images = detector.detect(exit_waves_stem).reduce_ensemble()\n",
    "haadf_images.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a20506-7155-45f0-b641-3931bbb20029",
   "metadata": {},
   "source": [
    "A benefit of using the distributed scheduler on a single machine is the live diagnostic dashboard. You can access this through the Dashboard link shown in the `__repr__` for the `Client` above (for details you can watch [this](https://www.youtube.com/watch?v=N_GqzcuGLCY) video walkthrough). If you are using Jupyter Lab, the [Dask labextension](https://github.com/dask/dask-labextension) provides the same information as a panel inside the Jupyter Lab editor.\n",
    "\n",
    "We can get back to the local scheduler by closing the `Client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38de7a7-2b82-442e-8258-d9e2717a7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe61f7-43d8-47e6-a00a-3e73e33320ad",
   "metadata": {},
   "source": [
    "### Running *ab*TEM on HPC clusters\n",
    "\n",
    "Dask (and thus *ab*TEM) has robust tools for deployment on high-performance compute clusters. We recommend consulting your HPC provider on how to deploy Dask applications on your available cluster. For general advice on deployment, please see the [Dask documentation](https://docs.dask.org/en/stable/deploying.html).\n",
    "\n",
    "#### Submitting job scripts \n",
    "\n",
    "As an overview, Dask provides a number of different cluster managers, so you can use distributed Dask on a range of platforms. These cluster managers deploy a scheduler and the necessary workers as determined by communicating with the resource manager. All cluster managers follow the same interface but have platform-specific configuration options.\n",
    "\n",
    "As an example, for deployment using [SLURM](https://slurm.schedmd.com/documentation.html), your script might look something like:\n",
    "\n",
    "```python\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    queue=\"regular\",\n",
    "    account=\"myaccount\",\n",
    "    cores=32,\n",
    "    memory=\"128 GB\"\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "# Your abTEM code goes here\n",
    "\n",
    "```\n",
    "\n",
    "Dask also supports deployment from within an existing MPI environment, such as one created with the common MPI command-line launcher `mpirun`, see [here](http://mpi.dask.org/en/latest/) for more information. You can turn your batch Python script into an MPI executable with the `dask_mpi.initialize` function. \n",
    "\n",
    "```python\n",
    "from dask_mpi import initialize\n",
    "initialize()\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()  # Connect this local process to remote workers\n",
    "\n",
    "# Your abTEM code goes here\n",
    "```\n",
    "\n",
    "This makes your Python script launchable directly with `mpirun`:\n",
    "```\n",
    "mpirun -np 4 python my_client_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cb8d2-a898-478d-9831-275ac5f2f06b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(walkthrough:parallelization:gpu)=\n",
    "## Using GPUs\n",
    "\n",
    "Almost every part of *ab*TEM can be accelerated using a GPU through the [CuPy](https://cupy.dev/) library. We have only tested CUDA-compatible GPUs, but any graphics card compatible with CuPy should work.\n",
    "\n",
    "If you have a compatible GPU and a working installation of CuPy, you can accelerate your image simulations by simply changing the configs at the top of your document as below:\n",
    "```python\n",
    "abtem.config.set({\"device\": \"gpu\"});\n",
    "```\n",
    "On a single GPU, you may also want to limit the number of workers to one, `num_workers=1`. As described above, this may be done through the Dask config:\n",
    "```python\n",
    "dask.config.set({\"num_workers\": 1});\n",
    "```\n",
    "\n",
    "Note that Dask does not manage GPU threads. This makes the choice of batch sizes (i.e. propagating multiple wave functions in a single batch) extremely important in order to fully utilize your GPU. The default batch size in GPU calculations in abTEM is $512\\ \\mathrm{MB}$, which is four times larger than the CPU batch size, but if your GPU has $8\\ \\mathrm{GB}$ or more memory, you will likely be able to increase this number to $2048\\ \\mathrm{MB}$.\n",
    "\n",
    "```python\n",
    "abtem.config.set({\"dask.chunk-size-gpu\" : \"512 MB\"})\n",
    "```\n",
    "\n",
    "```{note}\n",
    "The batch size only determines the maximum number of plane waves in a batch, so you need to leave room in the memory for any intermediate overhead. \n",
    "```\n",
    "\n",
    "Finally, *ab*TEM by default sets the [FFT plan cache](https://docs.cupy.dev/en/stable/user_guide/fft.html#fft-plan-cache) size of `cupy` to zero, as we find that in most cases the increased memory consumption of the plans are not worth the small speedup they provide. You can however also change this through the *ab*TEM config.\n",
    "\n",
    "```python\n",
    "abtem.config.set({\"cupy.fft-cache-size\" : \"1024 MB\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdb086-c8a9-4736-a345-311d47379f34",
   "metadata": {},
   "source": [
    "### Multiple GPUs \n",
    "The above is enough for running *ab*TEM on a single GPU; if you are using an NVidia GPU, you may want to install `dask_cuda` (this is currently only supported on Linux). However, `dask_cuda` is currently necessary for multi-GPU calculations with *ab*TEM.\n",
    "\n",
    "```python\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abtem-dask",
   "language": "python",
   "name": "abtem-dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
